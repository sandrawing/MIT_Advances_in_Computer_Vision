{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tuning Method.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK2yQHW1ymW9",
        "outputId": "1caa1278-fe64-4296-9427-f70c4eb3ce60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data directory on Tale's drive\n",
        "data_dir = '/content/drive/MyDrive/6.819_data'"
      ],
      "metadata": {
        "id": "nevmqNkG_MUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data directory on Sandra's drive\n",
        "data_dir = '/content/drive/MyDrive/CV_Final_Proj/code/data/unzip_data'"
      ],
      "metadata": {
        "id": "R2-gh4sJ_LoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "!pip install tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "import PIL \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import skimage\n",
        "  \n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only\")\n",
        "    print(\"You may want to try to use the GPU in Google Colab by clicking in:\")\n",
        "    print(\"Runtime > Change Runtime type > Hardware accelerator > GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIZVqdGo2Rdu",
        "outputId": "4e6ee66b-3761-4653-b814-19a6b83588e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Using the GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n",
        "                                     std=[0.33165374, 0.33165374, 0.33165374])\n",
        "train_transformer = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "#     transforms.RandomRotation(90),\n",
        "    # random brightness and random contrast\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_transformer = transforms.Compose([\n",
        "#     transforms.Resize(224),\n",
        "#     transforms.CenterCrop(224),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "metadata": {
        "id": "IiuXErKzGy7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Another way to process the dataset based on the txt split file\n",
        "## Consistent with the original paper\n",
        "\n",
        "batchsize=4\n",
        "def read_txt(txt_path):\n",
        "    with open(txt_path) as f:\n",
        "        lines = f.readlines()\n",
        "    txt_data = [line.strip() for line in lines]\n",
        "    return txt_data\n",
        "\n",
        "\n",
        "class CovidCTDataset(Dataset):\n",
        "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            txt_path (string): Path to the txt file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        File structure:\n",
        "        - root_dir\n",
        "            - CT_COVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "            - CT_NonCOVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
        "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
        "        self.num_cls = len(self.classes)\n",
        "        self.img_list = []\n",
        "        for c in range(self.num_cls):\n",
        "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
        "            self.img_list += cls_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.img_list[idx][0]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        sample = {'img': image,\n",
        "                  'label': int(self.img_list[idx][1])}\n",
        "        return sample"
      ],
      "metadata": {
        "id": "L_Jb2BbVC1-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = CovidCTDataset(root_dir=data_dir,\n",
        "                          txt_COVID='/content/drive/MyDrive/CV_Final_Proj/code/data/Data-split/COVID/trainCT_COVID.txt',\n",
        "                          txt_NonCOVID='/content/drive/MyDrive/CV_Final_Proj/code/data/Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
        "                          transform= train_transformer)\n",
        "valset = CovidCTDataset(root_dir=data_dir,\n",
        "                          txt_COVID='/content/drive/MyDrive/CV_Final_Proj/code/data/Data-split/COVID/valCT_COVID.txt',\n",
        "                          txt_NonCOVID='/content/drive/MyDrive/CV_Final_Proj/code/data/Data-split/NonCOVID/valCT_NonCOVID.txt',\n",
        "                          transform= val_transformer)\n",
        "testset = CovidCTDataset(root_dir=data_dir,\n",
        "                          txt_COVID='/content/drive/MyDrive/CV_Final_Proj/code/data/Data-split/COVID/testCT_COVID.txt',\n",
        "                          txt_NonCOVID='/content/drive/MyDrive/CV_Final_Proj/code/data/Data-split/NonCOVID/testCT_NonCOVID.txt',\n",
        "                          transform= val_transformer)\n",
        "print(trainset.__len__())\n",
        "print(valset.__len__())\n",
        "print(testset.__len__())\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n",
        "val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJCGOCZTHfJM",
        "outputId": "bb1015b7-fc58-4b99-ab19-ed01ea3fba9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "425\n",
            "118\n",
            "203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modeling"
      ],
      "metadata": {
        "id": "cz-gG1nD_dGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = None\n",
        "## alpha is None if mixup is not used\n",
        "alpha_name = f'{alpha}'\n",
        "device = 'cuda'\n",
        "\n",
        "def make_optimizer(model, learning_rate, print_parameters=False):\n",
        "    # Get all the parameters\n",
        "    params_to_update = model.parameters()\n",
        "    if print_parameters:\n",
        "      print(\"Params to learn:\")\n",
        "      for name, param in model.named_parameters():\n",
        "          if param.requires_grad == True:\n",
        "              print(\"\\t\",name)\n",
        "\n",
        " \n",
        "    optimizer = optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "4nIfPEJzG1xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### train function modified from problem sets\n",
        "def train_model(model, optimizer, epoch, save_dir = \"\", save_all_epochs=False):\n",
        "    '''\n",
        "    model: The NN to train\n",
        "    dataloaders: A dictionary containing at least the keys \n",
        "                 'train','val' that maps to Pytorch data loaders for the dataset\n",
        "    criterion: The Loss function\n",
        "    optimizer: The algorithm to update weights \n",
        "               (Variations on gradient descent)\n",
        "    num_epochs: How many epochs to train for\n",
        "    save_dir: Where to save the best model weights that are found, \n",
        "              as they are found. Will save to save_dir/weights_best.pt\n",
        "              Using None will not write anything to disk\n",
        "    save_all_epochs: Whether to save weights for ALL epochs, not just the best\n",
        "                     validation error epoch. Will save to save_dir/weights_e{#}.pt\n",
        "    '''\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "    train_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                data_loader = train_loader\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                data_loader = val_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            # TQDM has nice progress bars\n",
        "            for batch_index, batch_samples in enumerate(data_loader):\n",
        "        \n",
        "                # move data to device\n",
        "                inputs, labels = batch_samples['img'].to(device), batch_samples['label'].to(device) \n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    outputs = model(inputs)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # torch.max outputs the maximum value, and its index\n",
        "                    # Since the input is batched, we take the max along axis 1\n",
        "                    # (the meaningful outputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backprop + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(data_loader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(data_loader.dataset)\n",
        "            \n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'train':\n",
        "                train_acc_history.append(epoch_acc)\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "            if save_all_epochs:\n",
        "                torch.save(model.state_dict(), os.path.join(save_dir, f'weights_{epoch}.pt'))\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # save and load best model weights\n",
        "    torch.save(best_model_wts, os.path.join(save_dir, 'weights_best_val_acc.pt'))\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, 'weights_last.pt'.format(epoch)))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history, train_acc_history"
      ],
      "metadata": {
        "id": "FlD-A6FhOkpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, resume_from = None, use_pretrained = False):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    # The model (nn.Module) to return\n",
        "    model_ft = None\n",
        "    # The input image is expected to be (input_size, input_size)\n",
        "    input_size = 0\n",
        "    \n",
        "    # By default, all parameters will be trained (useful when you're starting from scratch)\n",
        "    # Within this function you can set .requires_grad = False for various parameters, if you\n",
        "    # don't want to learn them\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "        \n",
        "    elif model_name == \"resnet50\":\n",
        "        \"\"\" Resnet50\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        # if use_pretrained:\n",
        "        #   print('pretrained model')\n",
        "        #   for param in model_ft.features.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Invalid model name!\")\n",
        "    \n",
        "    if resume_from is not None:\n",
        "        print(\"Loading weights from %s\" % resume_from)\n",
        "        model_ft.load_state_dict(torch.load(resume_from))\n",
        "    \n",
        "    return model_ft, input_size"
      ],
      "metadata": {
        "id": "wEYwuI-m2c82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
        "# You can add your own, or modify these however you wish!\n",
        "model_name = 'resnet50'\n",
        "\n",
        "# Number of classes in the dataset, normal, benign, malignant\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "# Shuffle the input data?\n",
        "shuffle_datasets = True\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 20\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.002\n",
        "\n",
        "### IO\n",
        "# Path to a model file to use to start weights at\n",
        "resume_from = None\n",
        "\n",
        "# Whether to use a pretrained model, trained for classification in Imagenet-1k \n",
        "pretrained = True\n",
        "\n",
        "# Save all epochs so that you can select the model from a particular epoch\n",
        "save_all_epochs = False\n",
        "\n",
        "# Whether to use early stopping (load the model with best accuracy), or not\n",
        "early_stopping = True\n",
        "\n",
        "# Directory to save weights to\n",
        "# save_dir = models_dir + '/trained_model_1'\n",
        "# os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "PvWODWsCMx3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, is_labelled = False, generate_labels = True, k = 5):\n",
        "    # If is_labelled, we want to compute loss, top-1 accuracy and top-5 accuracy\n",
        "    # If generate_labels, we want to output the actual labels\n",
        "    # Set the model to evaluate mode\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    running_top1_correct = 0\n",
        "    running_top5_correct = 0\n",
        "    predicted_labels = []\n",
        "    gt_labels = []\n",
        "\n",
        "    # Iterate over data.\n",
        "    # TQDM has nice progress bars\n",
        "    for batch_index, batch_samples in enumerate(data_loader):\n",
        "        inputs, labels = batch_samples['img'].to(device), batch_samples['label'].to(device) \n",
        "        tiled_labels = torch.stack([labels.data for i in range(k)], dim=1) \n",
        "        # Makes this to calculate \"top 5 prediction is correct\"\n",
        "        # [[label1 label1 label1 label1 label1], [label2 label2 label2 label label2]]\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(False):\n",
        "            # Get model outputs and calculate loss\n",
        "            outputs = model(inputs)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            if is_labelled:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # torch.topk outputs the maximum values, and their indices\n",
        "            # Since the input is batched, we take the max along axis 1\n",
        "            # (the meaningful outputs)\n",
        "            _, preds = torch.topk(outputs, k=k, dim=1)\n",
        "            if generate_labels:\n",
        "                # We want to store these results\n",
        "                nparr = preds.cpu().detach().numpy()\n",
        "                predicted_labels.extend([list(nparr[i]) for i in range(len(nparr))])\n",
        "                gt_labels.extend(np.array(labels.cpu()))\n",
        "\n",
        "        if is_labelled:\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            # Check only the first prediction\n",
        "            running_top1_correct += torch.sum(preds[:, 0] == labels.data)\n",
        "            # Check all 5 predictions\n",
        "            running_top5_correct += torch.sum(preds == tiled_labels)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # Only compute loss & accuracy if we have the labels\n",
        "    if is_labelled:\n",
        "        epoch_loss = float(running_loss / len(data_loader.dataset))\n",
        "        epoch_top1_acc = float(running_top1_correct.double() / len(data_loader.dataset))\n",
        "        epoch_top5_acc = float(running_top5_correct.double() / len(data_loader.dataset))\n",
        "    else:\n",
        "        epoch_loss = None\n",
        "        epoch_top1_acc = None\n",
        "        epoch_top5_acc = None\n",
        "    \n",
        "    # Return everything\n",
        "    return epoch_loss, epoch_top1_acc, gt_labels, predicted_labels  "
      ],
      "metadata": {
        "id": "ALL3W45XWb74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eval_results(model, data_loader):\n",
        "    model.eval()\n",
        "    true_label_list = []\n",
        "    outputs_list = []\n",
        "    predicted_label_list = []\n",
        "    original_image_list = []\n",
        "\n",
        "    # TQDM has nice progress bars\n",
        "    for batch_index, batch_samples in enumerate(data_loader):\n",
        "        inputs, labels = batch_samples['img'].to(device), batch_samples['label'].to(device) \n",
        "        with torch.set_grad_enabled(False):\n",
        "            # Get model outputs and calculate loss\n",
        "            outputs = model(inputs)\n",
        "            true_label_list.append(labels)\n",
        "            original_image_list.append(inputs)\n",
        "            outputs_list.append(outputs)\n",
        "            _, preds = torch.topk(outputs, k=1, dim=1)\n",
        "            predicted_label_list.append(preds)\n",
        "    return torch.concat(true_label_list).unsqueeze(-1).cpu().numpy(), \\\n",
        "           torch.concat(predicted_label_list).cpu().numpy(), \\\n",
        "           torch.softmax(torch.concat(outputs_list), dim=1).cpu().numpy(), \\\n",
        "           torch.concat(original_image_list).cpu().numpy()"
      ],
      "metadata": {
        "id": "D8Szh3mfT-0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score"
      ],
      "metadata": {
        "id": "1vXmBkjmiQeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Resnet50"
      ],
      "metadata": {
        "id": "p4a2YL-q_Qh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, test_f1, test_auc = [], [], []\n",
        "\n",
        "for i in range(5):\n",
        "    model_1, input_size = initialize_model(model_name = 'resnet50', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    optimizer_1 = make_optimizer(model_1, learning_rate)\n",
        "    trained_model_1, validation_history_1, train_history_1 = train_model(model_1, optimizer_1, num_epochs)\n",
        "\n",
        "    # Load your final model, that we will use for the rest of the PSET.\n",
        "    if early_stopping:\n",
        "        weights_file = '' + 'weights_best_val_acc.pt'\n",
        "    else:\n",
        "        weights_file = '' + 'weights_last.pt'\n",
        "\n",
        "    model_1, _ = initialize_model(model_name = model_name, num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "\n",
        "    # Move the model to the gpu if needed\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    # Load weights for model_yours\n",
        "    model_1.load_state_dict(torch.load(weights_file))\n",
        "\n",
        "    # Get data on the validation set\n",
        "\n",
        "    val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model_1, val_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "    # Get predictions for the test set\n",
        "    test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model_1, test_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "\n",
        "    test_acc.append(test_top1_yours)\n",
        "\n",
        "    print(\"Our Trained model: \")\n",
        "    print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n",
        "    print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n",
        "\n",
        "    y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "    test_f1.append(f1_score(y_label, y_pred))\n",
        "    test_auc.append(roc_auc_score(y_label, outputs[:, 1]))\n",
        "\n",
        "    print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "    print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "xsP4jBV4hpA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(test_acc), np.std(test_acc))\n",
        "print(np.mean(test_f1), np.std(test_f1))\n",
        "print(np.mean(test_auc), np.std(test_auc))"
      ],
      "metadata": {
        "id": "37DTZxiGiwnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_label, y_pred, title='Confusion matrix'):\n",
        "    cf_matrix = np.zeros((2, 2))    \n",
        "    for actual_value, predicted_value in zip(y_label, y_pred):\n",
        "        cf_matrix[actual_value, predicted_value] += 1\n",
        "    cf_matrix_per = cf_matrix / np.sum(cf_matrix, axis=1, keepdims=True)\n",
        "    \n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix_per.flatten()]\n",
        "    labels = [f\"{v2}\\n{v1}\" for v1, v2 in\n",
        "              zip(group_counts, group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2, 2)\n",
        "\n",
        "    fig, ax = plt.subplots()  \n",
        "    im = ax.imshow(cf_matrix_per, cmap=\"Blues\")\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            text = ax.text(j, i, labels[i][j], ha=\"center\", va=\"center\", \n",
        "                            color='w' if cf_matrix_per[i, j] > 0.5 else 'black')\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_xticks(np.arange(2))\n",
        "    ax.set_yticks(np.arange(2))\n",
        "    ax.xaxis.set_ticklabels(['Non-COVID', 'COVID'])\n",
        "    ax.yaxis.set_ticklabels(['Non-COVID', 'COVID'])\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  \n",
        "plot_confusion_matrix(y_label, y_pred, 'Confusion Matrix - ResNet50')"
      ],
      "metadata": {
        "id": "mo7DiMCGUWe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Resnet18"
      ],
      "metadata": {
        "id": "Ap3TaqJr_Ulk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, test_f1, test_auc = [], [], []\n",
        "\n",
        "for i in range(5):\n",
        "    model_1, input_size = initialize_model(model_name = 'resnet', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    optimizer_1 = make_optimizer(model_1, learning_rate)\n",
        "    trained_model_1, validation_history_1, train_history_1 = train_model(model_1, optimizer_1, num_epochs)\n",
        "\n",
        "    # Load your final model, that we will use for the rest of the PSET.\n",
        "    if early_stopping:\n",
        "        weights_file = '' + 'weights_best_val_acc.pt'\n",
        "    else:\n",
        "        weights_file = '' + 'weights_last.pt'\n",
        "\n",
        "    model_1, _ = initialize_model(model_name = 'resnet', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "\n",
        "    # Move the model to the gpu if needed\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    # Load weights for model_yours\n",
        "    model_1.load_state_dict(torch.load(weights_file))\n",
        "\n",
        "    # Get data on the validation set\n",
        "\n",
        "    val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model_1, val_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "    # Get predictions for the test set\n",
        "    test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model_1, test_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "\n",
        "    test_acc.append(test_top1_yours)\n",
        "\n",
        "    print(\"Our Trained model: \")\n",
        "    print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n",
        "    print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n",
        "\n",
        "    y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "    test_f1.append(f1_score(y_label, y_pred))\n",
        "    test_auc.append(roc_auc_score(y_label, outputs[:, 1]))\n",
        "\n",
        "    print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "    print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "C8OcepldlLaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(test_acc), np.std(test_acc))\n",
        "print(np.mean(test_f1), np.std(test_f1))\n",
        "print(np.mean(test_auc), np.std(test_auc))"
      ],
      "metadata": {
        "id": "7R3hwwd_mXIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### densenet"
      ],
      "metadata": {
        "id": "tVtzpQwyElmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, test_f1, test_auc = [], [], []\n",
        "\n",
        "for i in range(5):\n",
        "    model_1, input_size = initialize_model(model_name = 'densenet', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    optimizer_1 = make_optimizer(model_1, learning_rate)\n",
        "    trained_model_1, validation_history_1, train_history_1 = train_model(model_1, optimizer_1, num_epochs)\n",
        "\n",
        "    # Load your final model, that we will use for the rest of the PSET.\n",
        "    if early_stopping:\n",
        "        weights_file = '' + 'weights_best_val_acc.pt'\n",
        "    else:\n",
        "        weights_file = '' + 'weights_last.pt'\n",
        "\n",
        "    model_1, _ = initialize_model(model_name = 'densenet', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "\n",
        "    # Move the model to the gpu if needed\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    # Load weights for model_yours\n",
        "    model_1.load_state_dict(torch.load(weights_file))\n",
        "\n",
        "    # Get data on the validation set\n",
        "\n",
        "    val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model_1, val_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "    # Get predictions for the test set\n",
        "    test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model_1, test_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "\n",
        "    test_acc.append(test_top1_yours)\n",
        "\n",
        "    print(\"Our Trained model: \")\n",
        "    print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n",
        "    print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n",
        "\n",
        "    y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "    test_f1.append(f1_score(y_label, y_pred))\n",
        "    test_auc.append(roc_auc_score(y_label, outputs[:, 1]))\n",
        "\n",
        "    print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "    print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "ElfDrL0xxRWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(test_acc), np.std(test_acc))\n",
        "print(np.mean(test_f1), np.std(test_f1))\n",
        "print(np.mean(test_auc), np.std(test_auc))"
      ],
      "metadata": {
        "id": "rn6UDnDx0rt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "plot_confusion_matrix(y_label, y_pred, 'Confusion Matrix - DenseNet121')\n",
        "print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "NoKLcKjqZVGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### vgg"
      ],
      "metadata": {
        "id": "rrNuiRBYFPqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, test_f1, test_auc = [], [], []\n",
        "\n",
        "for i in range(5):\n",
        "    model_1, input_size = initialize_model(model_name = 'vgg', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    optimizer_1 = make_optimizer(model_1, learning_rate)\n",
        "    trained_model_1, validation_history_1, train_history_1 = train_model(model_1, optimizer_1, num_epochs)\n",
        "\n",
        "    # Load your final model, that we will use for the rest of the PSET.\n",
        "    if early_stopping:\n",
        "        weights_file = '' + 'weights_best_val_acc.pt'\n",
        "    else:\n",
        "        weights_file = '' + 'weights_last.pt'\n",
        "\n",
        "    model_1, _ = initialize_model(model_name = 'vgg', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "\n",
        "    # Move the model to the gpu if needed\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    # Load weights for model_yours\n",
        "    model_1.load_state_dict(torch.load(weights_file))\n",
        "\n",
        "    # Get data on the validation set\n",
        "\n",
        "    val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model_1, val_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "    # Get predictions for the test set\n",
        "    test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model_1, test_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "\n",
        "    test_acc.append(test_top1_yours)\n",
        "\n",
        "    print(\"Our Trained model: \")\n",
        "    print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n",
        "    print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n",
        "\n",
        "    y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "    test_f1.append(f1_score(y_label, y_pred))\n",
        "    test_auc.append(roc_auc_score(y_label, outputs[:, 1]))\n",
        "\n",
        "    print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "    print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "k9_ZL4BB35oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(test_acc), np.std(test_acc))\n",
        "print(np.mean(test_f1), np.std(test_f1))\n",
        "print(np.mean(test_auc), np.std(test_auc))"
      ],
      "metadata": {
        "id": "K-fJpbFF35ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "plot_confusion_matrix(y_label, y_pred, 'Confusion Matrix - vgg')\n",
        "print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "3hsPVsHObdOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### AlexNet"
      ],
      "metadata": {
        "id": "pifp_hE6Gwo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, test_f1, test_auc = [], [], []\n",
        "\n",
        "for i in range(5):\n",
        "    model_1, input_size = initialize_model(model_name = 'alexnet', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    optimizer_1 = make_optimizer(model_1, learning_rate)\n",
        "    trained_model_1, validation_history_1, train_history_1 = train_model(model_1, optimizer_1, num_epochs)\n",
        "\n",
        "    # Load your final model, that we will use for the rest of the PSET.\n",
        "    if early_stopping:\n",
        "        weights_file = '' + 'weights_best_val_acc.pt'\n",
        "    else:\n",
        "        weights_file = '' + 'weights_last.pt'\n",
        "\n",
        "    model_1, _ = initialize_model(model_name = 'alexnet', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "\n",
        "    # Move the model to the gpu if needed\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    # Load weights for model_yours\n",
        "    model_1.load_state_dict(torch.load(weights_file))\n",
        "\n",
        "    # Get data on the validation set\n",
        "\n",
        "    val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model_1, val_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "    # Get predictions for the test set\n",
        "    test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model_1, test_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "\n",
        "    test_acc.append(test_top1_yours)\n",
        "\n",
        "    print(\"Our Trained model: \")\n",
        "    print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n",
        "    print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n",
        "\n",
        "    y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "    test_f1.append(f1_score(y_label, y_pred))\n",
        "    test_auc.append(roc_auc_score(y_label, outputs[:, 1]))\n",
        "\n",
        "    print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "    print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "nlIjxwEUEiHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(test_acc), np.std(test_acc))\n",
        "print(np.mean(test_f1), np.std(test_f1))\n",
        "print(np.mean(test_auc), np.std(test_auc))"
      ],
      "metadata": {
        "id": "PlHbVWpnElF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "plot_confusion_matrix(y_label, y_pred, 'Confusion Matrix - AlexNet')\n",
        "print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "zqyRq94oclCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### squeezenet"
      ],
      "metadata": {
        "id": "0yTiu7YhHL-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, test_f1, test_auc = [], [], []\n",
        "\n",
        "for i in range(5):\n",
        "    model_1, input_size = initialize_model(model_name = 'squeezenet', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    optimizer_1 = make_optimizer(model_1, learning_rate)\n",
        "    trained_model_1, validation_history_1, train_history_1 = train_model(model_1, optimizer_1, num_epochs)\n",
        "\n",
        "    # Load your final model, that we will use for the rest of the PSET.\n",
        "    if early_stopping:\n",
        "        weights_file = '' + 'weights_best_val_acc.pt'\n",
        "    else:\n",
        "        weights_file = '' + 'weights_last.pt'\n",
        "\n",
        "    model_1, _ = initialize_model(model_name = 'squeezenet', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "\n",
        "    # Move the model to the gpu if needed\n",
        "    model_1 = model_1.to(device)\n",
        "\n",
        "    # Load weights for model_yours\n",
        "    model_1.load_state_dict(torch.load(weights_file))\n",
        "\n",
        "    # Get data on the validation set\n",
        "\n",
        "    val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model_1, val_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "    # Get predictions for the test set\n",
        "    test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model_1, test_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "\n",
        "    test_acc.append(test_top1_yours)\n",
        "\n",
        "    print(\"Our Trained model: \")\n",
        "    print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n",
        "    print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n",
        "\n",
        "    y_label, y_pred, outputs, inputs =  get_eval_results(model_1, test_loader)\n",
        "    test_f1.append(f1_score(y_label, y_pred))\n",
        "    test_auc.append(roc_auc_score(y_label, outputs[:, 1]))\n",
        "\n",
        "    print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "    print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "NnoJyoZMH5B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(test_acc), np.std(test_acc))\n",
        "print(np.mean(test_f1), np.std(test_f1))\n",
        "print(np.mean(test_auc), np.std(test_auc))"
      ],
      "metadata": {
        "id": "00WXKBn6LNQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}