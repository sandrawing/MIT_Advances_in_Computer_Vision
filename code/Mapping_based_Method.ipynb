{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RPZzkgDoprta"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUUvfr-INdet"},"outputs":[],"source":["import logging\n","\n","logging.basicConfig(filename=\"test_log.txt\", level=logging.INFO, filemode=\"w\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1V860bTPqLxB"},"outputs":[],"source":["import os\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vt-2XB64qLuX"},"outputs":[],"source":["# Data directory on Sandra's drive\n","data_dir = 'unzip_data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNx2G7S3yufr","outputId":"c5c61961-04b3-458f-b126-b31c14c82c9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: torchvision==0.12.0 in /n/home02/xzeng/.local/lib/python3.8/site-packages (0.12.0)\n","Requirement already satisfied: torch==1.11.0 in /n/home02/xzeng/.local/lib/python3.8/site-packages (from torchvision==0.12.0) (1.11.0)\n","Requirement already satisfied: numpy in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (from torchvision==0.12.0) (1.19.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (from torchvision==0.12.0) (8.0.1)\n","Requirement already satisfied: requests in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (from torchvision==0.12.0) (2.24.0)\n","Requirement already satisfied: typing-extensions in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (from torchvision==0.12.0) (3.7.4.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (from requests->torchvision==0.12.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (from requests->torchvision==0.12.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (from requests->torchvision==0.12.0) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (from requests->torchvision==0.12.0) (1.25.11)\n"]}],"source":["!pip install torchvision==0.12.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53aHEcgqyufs","outputId":"8d449b93-e043-401d-90fa-7d6ce109fe71"},"outputs":[{"data":{"text/plain":["'1.11.0+cu102'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["torch.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExTYDzjpqLru","outputId":"21625f53-0612-40b9-99d0-aff412d2948a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: tqdm in /n/sw/eb/apps/centos7/Anaconda3/2020.11/lib/python3.8/site-packages (4.50.2)\n","Using the GPU!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","!pip install tqdm\n","from tqdm.notebook import tqdm\n","import os\n","import copy\n","import pandas as pd\n","import PIL \n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import skimage\n","  \n","# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(\"Using the GPU!\")\n","else:\n","    print(\"WARNING: Could not find GPU! Using CPU only\")\n","    print(\"You may want to try to use the GPU in Google Colab by clicking in:\")\n","    print(\"Runtime > Change Runtime type > Hardware accelerator > GPU.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iqe0poIaqLow"},"outputs":[],"source":["normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n","                                     std=[0.33165374, 0.33165374, 0.33165374])\n","train_transformer = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n","    transforms.RandomHorizontalFlip(),\n","#     transforms.RandomRotation(90),\n","    # random brightness and random contrast\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n","    transforms.ToTensor(),\n","    normalize\n","])\n","\n","val_transformer = transforms.Compose([\n","#     transforms.Resize(224),\n","#     transforms.CenterCrop(224),\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    normalize\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yo-aIgMUrX5J"},"outputs":[],"source":["## Another way to process the dataset based on the txt split file\n","## Consistent with the original paper\n","\n","batchsize=4\n","def read_txt(txt_path):\n","    with open(txt_path) as f:\n","        lines = f.readlines()\n","    txt_data = [line.strip() for line in lines]\n","    return txt_data\n","\n","\n","class CovidCTDataset(Dataset):\n","    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n","        \"\"\"\n","        Args:\n","            txt_path (string): Path to the txt file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        File structure:\n","        - root_dir\n","            - CT_COVID\n","                - img1.png\n","                - img2.png\n","                - ......\n","            - CT_NonCOVID\n","                - img1.png\n","                - img2.png\n","                - ......\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.txt_path = [txt_NonCOVID, txt_COVID]\n","        self.classes = ['CT_NonCOVID', 'CT_COVID']\n","        self.num_cls = len(self.classes)\n","        self.img_list = []\n","        for c in range(self.num_cls):\n","            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n","            self.img_list += cls_list\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_list)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_path = self.img_list[idx][0]\n","        image = Image.open(img_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","        sample = {'img': image,\n","                  'label': int(self.img_list[idx][1])}\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bfR_CCkrmuM","outputId":"e841a7eb-8e90-44c1-ae3e-55c7658f4bb3"},"outputs":[{"name":"stdout","output_type":"stream","text":["425\n","118\n","203\n"]}],"source":["trainset = CovidCTDataset(root_dir=data_dir,\n","                          txt_COVID='Data-split/COVID/trainCT_COVID.txt',\n","                          txt_NonCOVID='Data-split/NonCOVID/trainCT_NonCOVID.txt',\n","                          transform= train_transformer)\n","valset = CovidCTDataset(root_dir=data_dir,\n","                          txt_COVID='Data-split/COVID/valCT_COVID.txt',\n","                          txt_NonCOVID='Data-split/NonCOVID/valCT_NonCOVID.txt',\n","                          transform= val_transformer)\n","testset = CovidCTDataset(root_dir=data_dir,\n","                          txt_COVID='Data-split/COVID/testCT_COVID.txt',\n","                          txt_NonCOVID='Data-split/NonCOVID/testCT_NonCOVID.txt',\n","                          transform= val_transformer)\n","\n","print(trainset.__len__())\n","print(valset.__len__())\n","print(testset.__len__())\n","\n","train_loader = DataLoader(trainset, batch_size=batchsize, drop_last=False, shuffle=True)\n","val_loader = DataLoader(valset, batch_size=batchsize, drop_last=False, shuffle=False)\n","test_loader = DataLoader(testset, batch_size=batchsize, drop_last=False, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAa3MzCm902k"},"outputs":[],"source":["train_data = []\n","for i in range(trainset.__len__()):\n","   train_data.append([trainset.__getitem__(i)['img'], trainset.__getitem__(i)['label']])\n","\n","trainloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batchsize, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou2iCPVU-zj2"},"outputs":[],"source":["val_data = []\n","for i in range(valset.__len__()):\n","   val_data.append([valset.__getitem__(i)['img'], valset.__getitem__(i)['label']])\n","\n","valloader = torch.utils.data.DataLoader(val_data, shuffle=False, batch_size=batchsize, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YuA-5vcs-6tA"},"outputs":[],"source":["test_data = []\n","for i in range(testset.__len__()):\n","   test_data.append([testset.__getitem__(i)['img'], testset.__getitem__(i)['label']])\n","\n","testloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batchsize, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQUHYRsD1Udb"},"outputs":[],"source":["source_dataset = datasets.ImageFolder('additional_data', train_transformer)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Tgm8fc-rF7d","outputId":"13a4c471-0bb0-40df-ac1c-96e400cc0411"},"outputs":[{"data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 14137\n","    Root location: additional_data\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n","               RandomResizedCrop(size=(224, 224), scale=(0.5, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n","               RandomHorizontalFlip(p=0.5)\n","               ColorJitter(brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=None, hue=None)\n","               ToTensor()\n","               Normalize(mean=[0.45271412, 0.45271412, 0.45271412], std=[0.33165374, 0.33165374, 0.33165374])\n","           )"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["source_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfhcHJUOJmMd","outputId":"66255c2f-6e70-4cb0-bb41-26cdb8f63ffe"},"outputs":[{"data":{"text/plain":["['1NonCOVID', '2COVID']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["source_dataset.classes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qz-dQoTbQJn4","outputId":"c3155fc1-f1c7-4293-961a-2caf06118373"},"outputs":[{"data":{"text/plain":["{'1NonCOVID': 0, '2COVID': 1}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["source_dataset.class_to_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlQThzK_1bEJ"},"outputs":[],"source":["source_loader = torch.utils.data.DataLoader(source_dataset, batch_size=batchsize, shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIhRhnfnK76K","outputId":"1eb748b7-e218-465b-e979-56c9ad848a9e"},"outputs":[{"data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x2afca8faf550>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["source_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4nkrNnPrgzi"},"outputs":[],"source":["class MMD_loss(nn.Module):\n","    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5):\n","        super(MMD_loss, self).__init__()\n","        self.kernel_num = kernel_num\n","        self.kernel_mul = kernel_mul\n","        self.fix_sigma = None\n","        self.kernel_type = kernel_type\n","\n","    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n","        n_samples = int(source.size()[0]) + int(target.size()[0])\n","        total = torch.cat([source, target], dim=0)\n","        total0 = total.unsqueeze(0).expand(\n","            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n","        total1 = total.unsqueeze(1).expand(\n","            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n","        L2_distance = ((total0-total1)**2).sum(2)\n","        if fix_sigma:\n","            bandwidth = fix_sigma\n","        else:\n","            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n","        bandwidth /= kernel_mul ** (kernel_num // 2)\n","        bandwidth_list = [bandwidth * (kernel_mul**i)\n","                          for i in range(kernel_num)]\n","        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n","                      for bandwidth_temp in bandwidth_list]\n","        return sum(kernel_val)\n","\n","    def linear_mmd2(self, f_of_X, f_of_Y):\n","        loss = 0.0\n","        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n","        loss = delta.dot(delta.T)\n","        return loss\n","\n","    def forward(self, source, target):\n","        if self.kernel_type == 'linear':\n","            return self.linear_mmd2(source, target)\n","        elif self.kernel_type == 'rbf':\n","            batch_size = int(source.size()[0])\n","            kernels = self.guassian_kernel(\n","                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n","            XX = torch.mean(kernels[:batch_size, :batch_size])\n","            YY = torch.mean(kernels[batch_size:, batch_size:])\n","            XY = torch.mean(kernels[:batch_size, batch_size:])\n","            YX = torch.mean(kernels[batch_size:, :batch_size])\n","            loss = torch.mean(XX + YY - XY - YX)\n","            return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqTCC2MCrwa1"},"outputs":[],"source":["def CORAL(source, target):\n","    d = source.size(1)\n","    ns, nt = source.size(0), target.size(0)\n","\n","    # source covariance\n","    tmp_s = torch.ones((1, ns)).cuda() @ source\n","    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)\n","\n","    # target covariance\n","    tmp_t = torch.ones((1, nt)).cuda() @ target\n","    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)\n","\n","    # frobenius norm\n","    loss = (cs - ct).pow(2).sum().sqrt()\n","    loss = loss / (4 * d * d)\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1G76evNDggHI"},"outputs":[],"source":["class ResNet50Fc(nn.Module):\n","    def __init__(self):\n","        super(ResNet50Fc, self).__init__()\n","        model_resnet50 = models.resnet50(pretrained=True)\n","        self.newmodel = torch.nn.Sequential(*(list(model_resnet50.children())[:-1]))\n","        self.__in_features = model_resnet50.fc.in_features\n","\n","    def forward(self, x):\n","        x = self.newmodel(x)\n","        print(x.shape)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    def output_num(self):\n","        return self.__in_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8bOnPU5yufw"},"outputs":[],"source":["class ResNet101Fc(nn.Module):\n","    def __init__(self):\n","        super(ResNet101Fc, self).__init__()\n","        model_resnet101 = models.resnet101(pretrained=True)\n","        self.newmodel = torch.nn.Sequential(*(list(model_resnet101.children())[:-1]))\n","        self.__in_features = model_resnet101.fc.in_features\n","\n","    def forward(self, x):\n","        x = self.newmodel(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    def output_num(self):\n","        return self.__in_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zyjF9HXWyufw"},"outputs":[],"source":["class ResNet18Fc(nn.Module):\n","    def __init__(self):\n","        super(ResNet18Fc, self).__init__()\n","        model_resnet18 = models.resnet18(pretrained=True)\n","        self.newmodel = torch.nn.Sequential(*(list(model_resnet18.children())[:-1]))\n","        self.__in_features = model_resnet18.fc.in_features\n","\n","    def forward(self, x):\n","        x = self.newmodel(x)\n","        x = x.view(x.size(0), -1)\n","        return x\n","\n","    def output_num(self):\n","        return self.__in_features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DFlsT78cr6Nn"},"outputs":[],"source":["class TransferNet(nn.Module):\n","    def __init__(self,\n","                 num_class, \n","                 base_net='resnet18', \n","                 transfer_loss='mmd', \n","                 use_bottleneck=True, \n","                 bottleneck_width=256, \n","                 width=1024):\n","        super(TransferNet, self).__init__()\n","        if base_net == 'resnet50':\n","            self.base_network = ResNet50Fc()\n","        else:\n","            self.base_network = ResNet18Fc()\n","        self.use_bottleneck = use_bottleneck\n","        self.transfer_loss = transfer_loss\n","        bottleneck_list = [nn.Linear(self.base_network.output_num(\n","        ), bottleneck_width), nn.BatchNorm1d(bottleneck_width), nn.ReLU(), nn.Dropout(0.5)]\n","        self.bottleneck_layer = nn.Sequential(*bottleneck_list)\n","        # classifier_layer_list = [nn.Linear(self.base_network.output_num(), width), nn.ReLU(), nn.Dropout(0.5),\n","        #                          nn.Linear(width, num_class)]\n","        classifier_layer_list = [nn.Linear(self.base_network.output_num(), num_class)]\n","        self.classifier_layer = nn.Sequential(*classifier_layer_list)\n","\n","        self.bottleneck_layer[0].weight.data.normal_(0, 0.005)\n","        self.bottleneck_layer[0].bias.data.fill_(0.1)\n","        # for i in range(2):\n","        #     self.classifier_layer[i * 3].weight.data.normal_(0, 0.01)\n","        #     self.classifier_layer[i * 3].bias.data.fill_(0.0)\n","\n","    def forward(self, source, target):\n","        source = self.base_network(source)\n","        target = self.base_network(target)\n","        source_clf = self.classifier_layer(source)\n","        # add classification \n","        target_clf = self.classifier_layer(target)\n","        if self.use_bottleneck:\n","            source = self.bottleneck_layer(source)\n","            target = self.bottleneck_layer(target)\n","        transfer_loss = self.adapt_loss(source, target, self.transfer_loss)\n","        return source_clf, transfer_loss, target_clf\n","\n","    def predict(self, x):\n","        features = self.base_network(x)\n","        clf = self.classifier_layer(features)\n","        return clf\n","\n","    def adapt_loss(self, X, Y, adapt_loss):\n","        \"\"\"Compute adaptation loss, currently we support mmd and coral\n","\n","        Arguments:\n","            X {tensor} -- source matrix\n","            Y {tensor} -- target matrix\n","            adapt_loss {string} -- loss type, 'mmd' or 'coral'. You can add your own loss\n","\n","        Returns:\n","            [tensor] -- adaptation loss tensor\n","        \"\"\"\n","        if adapt_loss == 'mmd':\n","            mmd_loss = MMD_loss()\n","            loss = mmd_loss(X, Y)\n","        elif adapt_loss == 'coral':\n","            loss = CORAL(X, Y)\n","        else:\n","            # Your own loss\n","            loss = 0\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pG6f8_SsFV4"},"outputs":[],"source":["transfer_loss = 'coral'\n","learning_rate = 0.0001\n","n_class = 2\n","transfer_model = TransferNet(n_class, transfer_loss=transfer_loss, base_net='').cuda()\n","# optimizer = torch.optim.SGD([\n","#     {'params': transfer_model.base_network.parameters()},\n","#     {'params': transfer_model.bottleneck_layer.parameters(), 'lr': 10 * learning_rate},\n","#     {'params': transfer_model.classifier_layer.parameters(), 'lr': 10 * learning_rate},\n","# ], lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n","optimizer = optim.SGD(transfer_model.parameters(), lr=learning_rate, momentum=0.9)\n","lamb = 2.5\n","n_epoch = 40"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8K8MdWVkyufx"},"outputs":[],"source":["def get_eval_results(model, data_loader):\n","    model.eval()\n","    true_label_list = []\n","    outputs_list = []\n","    predicted_label_list = []\n","    original_image_list = []\n","\n","    # TQDM has nice progress bars\n","    for data, target in data_loader:\n","        inputs, labels = data.cuda(), target.cuda()\n","        with torch.set_grad_enabled(False):\n","            # Get model outputs and calculate loss\n","            outputs = model.predict(inputs)\n","            true_label_list.append(labels)\n","            original_image_list.append(inputs)\n","            outputs_list.append(outputs)\n","            _, preds = torch.topk(outputs, k=1, dim=1)\n","            predicted_label_list.append(preds)\n","    return torch.concat(true_label_list).unsqueeze(-1).cpu().numpy(), \\\n","           torch.concat(predicted_label_list).cpu().numpy(), \\\n","           torch.softmax(torch.concat(outputs_list), dim=1).cpu().numpy(), \\\n","           torch.concat(original_image_list).cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yQzVTQD4yufx"},"outputs":[],"source":["def evaluate(model, data_loader, is_labelled = False, generate_labels = True, k = 5):\n","    # If is_labelled, we want to compute loss, top-1 accuracy and top-5 accuracy\n","    # If generate_labels, we want to output the actual labels\n","    # Set the model to evaluate mode\n","    model.eval()\n","    running_loss = 0\n","    running_top1_correct = 0\n","    running_top5_correct = 0\n","    predicted_labels = []\n","    gt_labels = []\n","\n","    # Iterate over data.\n","    # TQDM has nice progress bars\n","    for data, target in data_loader:\n","        inputs, labels = data.cuda(), target.cuda()\n","        tiled_labels = torch.stack([labels.data for i in range(k)], dim=1) \n","        # Makes this to calculate \"top 5 prediction is correct\"\n","        # [[label1 label1 label1 label1 label1], [label2 label2 label2 label label2]]\n","\n","        # forward\n","        # track history if only in train\n","        with torch.set_grad_enabled(False):\n","            # Get model outputs and calculate loss\n","            outputs = model.predict(inputs)\n","            criterion = nn.CrossEntropyLoss()\n","            if is_labelled:\n","                loss = criterion(outputs, labels)\n","\n","            # torch.topk outputs the maximum values, and their indices\n","            # Since the input is batched, we take the max along axis 1\n","            # (the meaningful outputs)\n","            _, preds = torch.topk(outputs, k=k, dim=1)\n","            if generate_labels:\n","                # We want to store these results\n","                nparr = preds.cpu().detach().numpy()\n","                predicted_labels.extend([list(nparr[i]) for i in range(len(nparr))])\n","                gt_labels.extend(np.array(labels.cpu()))\n","\n","        if is_labelled:\n","            # statistics\n","            running_loss += loss.item() * inputs.size(0)\n","            # Check only the first prediction\n","            running_top1_correct += torch.sum(preds[:, 0] == labels.data)\n","            # Check all 5 predictions\n","            running_top5_correct += torch.sum(preds == tiled_labels)\n","        else:\n","            pass\n","\n","    # Only compute loss & accuracy if we have the labels\n","    if is_labelled:\n","        epoch_loss = float(running_loss / len(data_loader.dataset))\n","        epoch_top1_acc = float(running_top1_correct.double() / len(data_loader.dataset))\n","        epoch_top5_acc = float(running_top5_correct.double() / len(data_loader.dataset))\n","    else:\n","        epoch_loss = None\n","        epoch_top1_acc = None\n","        epoch_top5_acc = None\n","    \n","    # Return everything\n","    return epoch_loss, epoch_top1_acc, gt_labels, predicted_labels  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOIAMS6vsdW0"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","early_stop = 20\n","dataloaders = {'src': source_loader, 'val': trainloader, 'tar': valloader}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0oIqcP-WFV_5"},"outputs":[],"source":["def test(model, target_test_loader):\n","    model.eval()\n","    correct = 0\n","    len_target_dataset = len(target_test_loader.dataset)\n","    with torch.no_grad():\n","        for data, target in target_test_loader:\n","            data, target = data.cuda(), target.cuda()\n","            s_output = model.predict(data)\n","            pred = torch.max(s_output, 1)[1]\n","            correct += torch.sum(pred == target)\n","    acc = correct.double() / len(target_test_loader.dataset)\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BlrwrgwSovS"},"outputs":[],"source":["CUDA_LAUNCH_BLOCKING=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DhX51yu6sFS9"},"outputs":[],"source":["def train(dataloaders, model, optimizer):\n","    source_loader, target_train_loader, target_test_loader = dataloaders['src'], dataloaders['val'], dataloaders['tar']\n","    len_source_loader = len(source_loader)\n","    len_target_loader = len(target_train_loader)\n","    best_acc = 0\n","    stop = 0\n","    n_batch = min(len_source_loader, len_target_loader)\n","    for e in range(n_epoch):\n","        stop += 1\n","        train_loss_clf, train_loss_transfer, train_loss_total, target_loss_ = 0, 0, 0, 0\n","        model.train()\n","        for (src, tar) in zip(source_loader, target_train_loader):\n","            data_source, label_source = src\n","            data_target, label_target = tar\n","            data_source, label_source = data_source.cuda(), label_source.cuda()\n","            data_target, label_target = data_target.cuda(), label_target.cuda()\n","\n","            optimizer.zero_grad()\n","            label_source_pred, transfer_loss, target_clf = model(data_source, data_target)\n","            clf_loss = criterion(label_source_pred, label_source)\n","            # add another loss\n","            clf_loss_target = criterion(target_clf, label_target)\n","\n","            loss = clf_loss + lamb * transfer_loss + clf_loss_target\n","            # loss = clf_loss + lamb * transfer_loss\n","            loss.backward()\n","            optimizer.step()\n","            train_loss_clf = clf_loss.detach().item() + train_loss_clf\n","            train_loss_transfer = transfer_loss.detach().item() + train_loss_transfer\n","            train_loss_total = loss.detach().item() + train_loss_total\n","            target_loss_ = clf_loss_target.detach().item() + target_loss_\n","        acc = test(model, target_test_loader)\n","        logging.info(f'Epoch: [{e:2d}/{n_epoch}], target_loss_cls_:{target_loss_/n_batch:.4f}, loss: {train_loss_clf/n_batch:.4f}, transfer_loss: {train_loss_transfer/n_batch:.4f}, total_Loss: {train_loss_total/n_batch:.4f}, acc: {acc:.4f}')\n","        if best_acc < acc:\n","            best_acc = acc\n","            torch.save(model.state_dict(), 'trans_model.pkl')\n","            stop = 0\n","        if stop >= early_stop:\n","            break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9xWeDYOyufy"},"outputs":[],"source":["def calc_prec_recall(y_true, y_pred):\n","    TP = ((y_true == 1) & (y_pred == 1)).sum()\n","    TN = ((y_true != 1) & (y_pred != 1)).sum()\n","    FP = ((y_pred == 1) & (y_true != 1)).sum()\n","    FN = ((y_pred != 1) & (y_true == 1)).sum()\n","    \n","    if TP + FP > 0:\n","        precision = TP / (TP + FP)\n","    else:\n","        precision = 1\n","    \n","    if TP + FN > 0:\n","        recall = TP / (TP + FN)\n","    else:\n","        recall = 1\n","\n","    return precision, recall\n","\n","\n","def calc_f1_auc(y_true, y_pred):\n","    precision, recall = calc_prec_recall(y_label, y_pred)\n","    f1 = 2 * (precision * recall) / (precision + recall)\n","    \n","    probability_thresholds = np.linspace(0, 1, num=100, endpoint=True)\n","\n","    precision_scores, recall_scores = [], []\n","    for p in probability_thresholds:\n","\n","        y_new_pred = np.zeros(y_pred.shape)\n","        y_new_pred[(outputs[:, 1] > p)] = 1\n","\n","        precision, recall = calc_prec_recall(y_label, y_new_pred)\n","\n","        precision_scores.append(precision)\n","        recall_scores.append(recall)\n","\n","    area_val = 0\n","    for i in range(1, len(precision_scores)):\n","        area_val += abs((precision_scores[i] + precision_scores[i-1]) * (recall_scores[i] - recall_scores[i-1])  / 2)\n","    \n","    return f1, area_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NWTah6uyufz"},"outputs":[],"source":["logging.info(\"resnet 18, transfer_loss = CORAL, learning_rate = 0.0001, lamb = 2.5, n_epoch = 40, loss=clf_loss + lamb * transfer_loss + clf_loss_target\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOxAQ_txyufz"},"outputs":[],"source":["test_acc, test_f1, test_auc = [], [], []\n","\n","for i in range(4):\n","    train(dataloaders, transfer_model, optimizer)\n","    transfer_model.load_state_dict(torch.load('trans_model.pkl'))\n","    acc_test = test(transfer_model, testloader)\n","    logging.info(f'Test accuracy: {acc_test}')\n","    test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(transfer_model, testloader, is_labelled = True, generate_labels = True, k = 1)\n","\n","    print(\"Our Trained model: \")\n","    print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n","    test_acc.append(test_top1_yours)\n","\n","    y_label, y_pred, outputs, inputs =  get_eval_results(transfer_model, testloader)\n","    f1, auc = calc_f1_auc(y_label, y_pred)\n","    \n","    test_f1.append(f1)\n","    test_auc.append(auc)\n","\n","    print(\"f1 score is :\", f1)\n","    print(\"AUC score is \", auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qF817EFiyufz"},"outputs":[],"source":["logging.info(f'Test accuracy: {np.mean(test_acc):.4f}, f1 score:{np.mean(test_f1):.4f}, AUC score:{np.mean(test_auc):.4f}')\n","logging.info(f'std accuracy: {np.std(test_acc):.4f}, f1 score:{np.std(test_f1):.4f}, AUC score:{np.std(test_auc):.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyWhT1-U8vEw"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Mapping_based_Method.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}