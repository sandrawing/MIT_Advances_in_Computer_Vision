{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG_finetuning_WnB.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcoC358XSwiG",
        "outputId": "92b42814-cb35-4de5-bb9e-3f9e1c730e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data directory on Tale's drive\n",
        "data_dir = '/content/drive/MyDrive/6.819_data'"
      ],
      "metadata": {
        "id": "CwQUJ00OS7O2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "!pip install tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import copy\n",
        "import pandas as pd\n",
        "import PIL \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import skimage\n",
        "  \n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only\")\n",
        "    print(\"You may want to try to use the GPU in Google Colab by clicking in:\")\n",
        "    print(\"Runtime > Change Runtime type > Hardware accelerator > GPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V2WD9LmTAES",
        "outputId": "29faccee-288f-41dc-e630-71c3f707d156"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Using the GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g--g053KTBvf",
        "outputId": "25883d6a-eab4-4eae-99a3-4de9b5aa7d8b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.11)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.9)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "5T0-vYBZTTMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "jmUSnrWMXAZx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"grid\",\n",
        "}"
      ],
      "metadata": {
        "id": "mt8r_zOhTWgz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'\n",
        "}\n",
        "\n",
        "sweep_config['metric'] = metric"
      ],
      "metadata": {
        "id": "Hw_HN_QXVJGz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_dict = {\n",
        "    'learning_rate': {'values': [0.001,0.002,0.004,0.008,0.01]\n",
        "    },\n",
        "    'batch_size': {'values': [16,32,64,128]\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config[\"parameters\"] = parameters_dict"
      ],
      "metadata": {
        "id": "sweQ5nV3VcYk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project = \"6.819_proj\")"
      ],
      "metadata": {
        "id": "1p0yW3e0W-52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ],
      "metadata": {
        "id": "upWmVSyPXMGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize(mean=[0.45271412, 0.45271412, 0.45271412],\n",
        "                                     std=[0.33165374, 0.33165374, 0.33165374])\n",
        "train_transformer = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomResizedCrop((224),scale=(0.5,1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "#     transforms.RandomRotation(90),\n",
        "    # random brightness and random contrast\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "val_transformer = transforms.Compose([\n",
        "#     transforms.Resize(224),\n",
        "#     transforms.CenterCrop(224),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "metadata": {
        "id": "ya0jOJPsYQyl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Another way to process the dataset based on the txt split file\n",
        "## Consistent with the original paper\n",
        "\n",
        "#batchsize=4\n",
        "def read_txt(txt_path):\n",
        "    with open(txt_path) as f:\n",
        "        lines = f.readlines()\n",
        "    txt_data = [line.strip() for line in lines]\n",
        "    return txt_data\n",
        "\n",
        "\n",
        "class CovidCTDataset(Dataset):\n",
        "    def __init__(self, root_dir, txt_COVID, txt_NonCOVID, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            txt_path (string): Path to the txt file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        File structure:\n",
        "        - root_dir\n",
        "            - COVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "            - NonCOVID\n",
        "                - img1.png\n",
        "                - img2.png\n",
        "                - ......\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.txt_path = [txt_COVID,txt_NonCOVID]\n",
        "        self.classes = ['COVID', 'NonCOVID']\n",
        "        self.num_cls = len(self.classes)\n",
        "        self.img_list = []\n",
        "        for c in range(self.num_cls):\n",
        "            cls_list = [[os.path.join(self.root_dir,self.classes[c],item), c] for item in read_txt(self.txt_path[c])]\n",
        "            self.img_list += cls_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_path = self.img_list[idx][0]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        sample = {'img': image,\n",
        "                  'label': int(self.img_list[idx][1])}\n",
        "        return sample"
      ],
      "metadata": {
        "id": "VSOkVdbVYs4d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_model(model_name, num_classes, resume_from = None, use_pretrained = False):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    # The model (nn.Module) to return\n",
        "    model_ft = None\n",
        "    # The input image is expected to be (input_size, input_size)\n",
        "    input_size = 0\n",
        "    \n",
        "    # By default, all parameters will be trained (useful when you're starting from scratch)\n",
        "    # Within this function you can set .requires_grad = False for various parameters, if you\n",
        "    # don't want to learn them\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "        \n",
        "    elif model_name == \"resnet50\":\n",
        "        \"\"\" Resnet50\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        # if use_pretrained:\n",
        "        #   print('pretrained model')\n",
        "        #   for param in model_ft.features.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
        "        input_size = 224\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Invalid model name!\")\n",
        "    \n",
        "    if resume_from is not None:\n",
        "        print(\"Loading weights from %s\" % resume_from)\n",
        "        model_ft.load_state_dict(torch.load(resume_from))\n",
        "    \n",
        "    return model_ft, input_size"
      ],
      "metadata": {
        "id": "fcyNfOj9aJ54"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, is_labelled = False, generate_labels = True, k = 5):\n",
        "    # If is_labelled, we want to compute loss, top-1 accuracy and top-5 accuracy\n",
        "    # If generate_labels, we want to output the actual labels\n",
        "    # Set the model to evaluate mode\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    running_top1_correct = 0\n",
        "    running_top5_correct = 0\n",
        "    predicted_labels = []\n",
        "    gt_labels = []\n",
        "\n",
        "    # Iterate over data.\n",
        "    # TQDM has nice progress bars\n",
        "    for batch_index, batch_samples in enumerate(data_loader):\n",
        "        inputs, labels = batch_samples['img'].to(device), batch_samples['label'].to(device) \n",
        "        tiled_labels = torch.stack([labels.data for i in range(k)], dim=1) \n",
        "        # Makes this to calculate \"top 5 prediction is correct\"\n",
        "        # [[label1 label1 label1 label1 label1], [label2 label2 label2 label label2]]\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(False):\n",
        "            # Get model outputs and calculate loss\n",
        "            outputs = model(inputs)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            if is_labelled:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # torch.topk outputs the maximum values, and their indices\n",
        "            # Since the input is batched, we take the max along axis 1\n",
        "            # (the meaningful outputs)\n",
        "            _, preds = torch.topk(outputs, k=k, dim=1)\n",
        "            if generate_labels:\n",
        "                # We want to store these results\n",
        "                nparr = preds.cpu().detach().numpy()\n",
        "                predicted_labels.extend([list(nparr[i]) for i in range(len(nparr))])\n",
        "                gt_labels.extend(np.array(labels.cpu()))\n",
        "\n",
        "        if is_labelled:\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            # Check only the first prediction\n",
        "            running_top1_correct += torch.sum(preds[:, 0] == labels.data)\n",
        "            # Check all 5 predictions\n",
        "            running_top5_correct += torch.sum(preds == tiled_labels)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # Only compute loss & accuracy if we have the labels\n",
        "    if is_labelled:\n",
        "        epoch_loss = float(running_loss / len(data_loader.dataset))\n",
        "        epoch_top1_acc = float(running_top1_correct.double() / len(data_loader.dataset))\n",
        "        epoch_top5_acc = float(running_top5_correct.double() / len(data_loader.dataset))\n",
        "    else:\n",
        "        epoch_loss = None\n",
        "        epoch_top1_acc = None\n",
        "        epoch_top5_acc = None\n",
        "    \n",
        "    # Return everything\n",
        "    return epoch_loss, epoch_top1_acc, gt_labels, predicted_labels"
      ],
      "metadata": {
        "id": "CZwmmhO0aSGV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eval_results(model, data_loader):\n",
        "    model.eval()\n",
        "    true_label_list = []\n",
        "    outputs_list = []\n",
        "    predicted_label_list = []\n",
        "    original_image_list = []\n",
        "\n",
        "    # TQDM has nice progress bars\n",
        "    for batch_index, batch_samples in enumerate(data_loader):\n",
        "        inputs, labels = batch_samples['img'].to(device), batch_samples['label'].to(device) \n",
        "        with torch.set_grad_enabled(False):\n",
        "            # Get model outputs and calculate loss\n",
        "            outputs = model(inputs)\n",
        "            true_label_list.append(labels)\n",
        "            original_image_list.append(inputs)\n",
        "            outputs_list.append(outputs)\n",
        "            _, preds = torch.topk(outputs, k=1, dim=1)\n",
        "            predicted_label_list.append(preds)\n",
        "    return torch.concat(true_label_list).unsqueeze(-1).cpu().numpy(), \\\n",
        "           torch.concat(predicted_label_list).cpu().numpy(), \\\n",
        "           torch.softmax(torch.concat(outputs_list), dim=1).cpu().numpy(), \\\n",
        "           torch.concat(original_image_list).cpu().numpy()"
      ],
      "metadata": {
        "id": "nefvo-3jaUJY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, f1_score"
      ],
      "metadata": {
        "id": "qFqSRmmBaX3n"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet]\n",
        "# You can add your own, or modify these however you wish!\n",
        "# Number of classes in the dataset, normal, benign, malignant\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "#batch_size = 32\n",
        "\n",
        "# Shuffle the input data?\n",
        "shuffle_datasets = True\n",
        "\n",
        "# Number of epochs to train for \n",
        "num_epochs = 20\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.002\n",
        "\n",
        "### IO\n",
        "# Path to a model file to use to start weights at\n",
        "resume_from = None\n",
        "\n",
        "# Whether to use a pretrained model, trained for classification in Imagenet-1k \n",
        "pretrained = True\n",
        "\n",
        "# Save all epochs so that you can select the model from a particular epoch\n",
        "save_all_epochs = False\n",
        "\n",
        "# Whether to use early stopping (load the model with best accuracy), or not\n",
        "early_stopping = True\n",
        "\n",
        "# Directory to save weights to\n",
        "# save_dir = models_dir + '/trained_model_1'\n",
        "# os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "IyifjZEwc5mV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = CovidCTDataset(root_dir=data_dir+'/train',\n",
        "                          txt_COVID='drive/MyDrive/6.819_data/train_COVID.txt',\n",
        "                          txt_NonCOVID='drive/MyDrive/6.819_data/train_NON_COVID.txt',\n",
        "                          transform= train_transformer)\n",
        "valset = CovidCTDataset(root_dir=data_dir+'/val',\n",
        "                          txt_COVID='drive/MyDrive/6.819_data/val_COVID.txt',\n",
        "                          txt_NonCOVID='drive/MyDrive/6.819_data/val_NON_COVID.txt',\n",
        "                          transform= val_transformer)\n",
        "testset = CovidCTDataset(root_dir=data_dir+'/test',\n",
        "                          txt_COVID='drive/MyDrive/6.819_data/test_COVID.txt',\n",
        "                          txt_NonCOVID='drive/MyDrive/6.819_data/test_NON_COVID.txt',\n",
        "                          transform= val_transformer)\n",
        "print(trainset.__len__())\n",
        "print(valset.__len__())\n",
        "print(testset.__len__())"
      ],
      "metadata": {
        "id": "u5tq7B4PZA5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = None\n",
        "## alpha is None if mixup is not used\n",
        "alpha_name = f'{alpha}'\n",
        "device = 'cuda'\n",
        "\n",
        "def make_optimizer(model, learning_rate, print_parameters=False):\n",
        "    # Get all the parameters\n",
        "    params_to_update = model.parameters()\n",
        "    if print_parameters:\n",
        "      print(\"Params to learn:\")\n",
        "      for name, param in model.named_parameters():\n",
        "          if param.requires_grad == True:\n",
        "              print(\"\\t\",name)\n",
        "\n",
        " \n",
        "    optimizer = optim.SGD(params_to_update, lr=learning_rate, momentum=0.9)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "N0VqnWipZQok"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    with wandb.init(config = None):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_loader = DataLoader(trainset, batch_size=config.batch_size, drop_last=False, shuffle=True)\n",
        "        val_loader = DataLoader(valset, batch_size=config.batch_size, drop_last=False, shuffle=False)\n",
        "        test_loader = DataLoader(testset, batch_size=config.batch_size, drop_last=False, shuffle=False)\n",
        "\n",
        "        model, input_size = initialize_model(model_name = 'vgg', num_classes = num_classes, resume_from=resume_from, use_pretrained=pretrained)\n",
        "        model = model.to(device)\n",
        "\n",
        "        optimizer = make_optimizer(model, config.learning_rate)\n",
        "        for epoch in range(num_epochs):\n",
        "            print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    cum_sum = 0\n",
        "                    model.train()  # Set model to training mode\n",
        "                    data_loader = train_loader\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "                    data_loader = val_loader\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                # TQDM has nice progress bars\n",
        "                for batch_index, batch_samples in enumerate(data_loader):\n",
        "            \n",
        "                    # move data to device\n",
        "                    inputs, labels = batch_samples['img'].to(device), batch_samples['label'].to(device) \n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # Get model outputs and calculate loss\n",
        "                        outputs = model(inputs)\n",
        "                        criterion = nn.CrossEntropyLoss()\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # torch.max outputs the maximum value, and its index\n",
        "                        # Since the input is batched, we take the max along axis 1\n",
        "                        # (the meaningful outputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                        # backprop + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            cum_sum += loss.item()\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                wandb.log({'loss': cum_sum/len(train_loader)})\n",
        "\n",
        "\n",
        "\n",
        "        val_loss_yours, val_top1_yours, _, val_labels_yours = evaluate(model, val_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "        # Get predictions for the test set\n",
        "        test_loss_yours, test_top1_yours, _, test_labels_yours = evaluate(model, test_loader, is_labelled = True, generate_labels = True, k = 1)\n",
        "\n",
        "        print(\"Our Trained model: \")\n",
        "        print(\"Val Top-1 Accuracy: {}\".format(val_top1_yours))\n",
        "        print(\"Test Top-1 Accuracy: {}\".format(test_top1_yours))\n",
        "\n",
        "        #print(\"f1 score is :\", f1_score(y_label, y_pred))\n",
        "        #print(\"AUC score is \", roc_auc_score(y_label, outputs[:, 1]))"
      ],
      "metadata": {
        "id": "-0_yztYsalF6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train,count=20)"
      ],
      "metadata": {
        "id": "agJmdSzMb4LX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}